{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGBJegvBuXV"
   },
   "source": [
    "# Assignment: Video Quality Inference\n",
    "\n",
    "To this point in the class, you have learned various techniques for leading and analyzing packet captures of various types, generating features from those packet captures, and training and evaluating models using those features.\n",
    "\n",
    "In this assignment, you will put all of this together, using a network traffic trace to train a model to automatically infer video quality of experience from a labeled traffic trace.\n",
    "\n",
    "## Part 1: Warmup\n",
    "\n",
    "The first part of this assignment builds directly on the hands-on activities but extends them slightly.\n",
    "\n",
    "### Extract Features from the Network Traffic\n",
    "\n",
    "Load the `netflix.pcap` file, which is a packet trace that includes network traffic. \n",
    "\n",
    "Click [here](https://github.com/noise-lab/ml-systems/blob/main/docs/notebooks/data/netflix.pcap) to download `netflix.pcap`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from netml.pparser.parser import PCAP\n",
    "import ipaddress\n",
    "\n",
    "pc = PCAP(pcap_file=\"netflix.pcap\")\n",
    "pc.pcap2pandas()\n",
    "df = pc.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOPPsKpYB6PS"
   },
   "source": [
    "### Identifying the Service Type\n",
    "\n",
    "Use the DNS traffic to filter the packet trace for Netflix traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_DOMAINS = [\"nflxvideo\",\"netflix\",\"nflxso\",\"nflxext\"]\n",
    "nfre = '|'.join(NF_DOMAINS)\n",
    "nf_dns = df[(df[\"is_dns\"]) & (df[\"dns_query\"].astype(str).str.contains(nfre, regex=True, case=False, na=False))]\n",
    "nf_dns.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "### Generate Statistics\n",
    "\n",
    "Generate statistics and features for the Netflix traffic flows. Use the `netml` library or any other technique that you choose to generate a set of features that you think would be good features for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_ips(cell):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    \n",
    "    out = []\n",
    "    text = str(cell)\n",
    "\n",
    "    matches = re.findall(r'[A-Fa-f0-9:.]+', text) # Match v4 and v6\n",
    "\n",
    "    for x in matches:\n",
    "        try:\n",
    "            ip = ipaddress.ip_address(x)\n",
    "            out.append(str(ip))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return out\n",
    "\n",
    "nf_ips = set()\n",
    "for resp in nf_dns[\"dns_resp\"]:\n",
    "    if pd.notna(resp):\n",
    "        ips = get_ips(resp)\n",
    "        nf_ips.update(ips)\n",
    "\n",
    "nf = df[df[\"ip_src\"].isin(nf_ips) | df[\"ip_dst\"].isin(nf_ips)].copy()\n",
    "nf[\"datetime\"] = pd.to_datetime(nf[\"datetime\"])\n",
    "\n",
    "flow_stats = (\n",
    "    # group by flows\n",
    "    nf.groupby([\"ip_src\",\"ip_dst\",\"port_src\",\"port_dst\",\"protocol\"]).agg(\n",
    "        flow_start=(\"datetime\",\"min\"),\n",
    "        flow_end=(\"datetime\",\"max\"),\n",
    "        n_packets=(\"length\",\"count\"),\n",
    "        n_bytes=(\"length\",\"sum\"),\n",
    "        mean_pkt_size=(\"length\",\"mean\"),\n",
    "        std_pkt_size=(\"length\",\"std\"),\n",
    "    ).reset_index()\n",
    ")\n",
    "\n",
    "flow_stats[\"duration_s\"] = (flow_stats[\"flow_end\"] - flow_stats[\"flow_start\"]).dt.total_seconds()\n",
    "flow_stats[\"bytes_per_sec\"] = flow_stats[\"n_bytes\"] / flow_stats[\"duration_s\"]\n",
    "flow_stats[\"pkts_per_sec\"] = flow_stats[\"n_packets\"] / flow_stats[\"duration_s\"]\n",
    "\n",
    "reverse_bytes = {\n",
    "    (r.ip_dst, r.ip_src, r.port_dst, r.port_src, r.protocol): r.n_bytes\n",
    "    for _, r in flow_stats.iterrows()\n",
    "}\n",
    "\n",
    "ratios=[]\n",
    "for _, r in flow_stats.iterrows():\n",
    "    key_rev = (r.ip_dst, r.ip_src, r.port_dst, r.port_src, r.protocol)\n",
    "    rev_val = reverse_bytes.get(key_rev, None)\n",
    "    if rev_val and rev_val > 0:\n",
    "        ratios.append(max(r.n_bytes, rev_val) / rev_val)\n",
    "    else:\n",
    "        ratios.append(pd.NA)\n",
    "\n",
    "flow_stats[\"down_up_ratio\"] = ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "**Write a brief justification for the features that you have chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selected are based on video streaming characteristics.\n",
    "Flow duration: Video streaming sessions are long-lived flows. DNS, etc. are short.\n",
    "No. of packets/bytes: How sustained the streaming was?\n",
    "Mean and std. packet size: Server->Client downstream packets are large (mostly MTU sized). \n",
    "Bytes/second nad packets/second: Throughput. Its a quality metric important for streaming.\n",
    "Downstream/upstream ratio: Downstream dominates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBCP_2SBC2xj"
   },
   "source": [
    "### Inferring Segment downloads\n",
    "\n",
    "In addition to the features that you could generate using the `netml` library or similar, add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "\n",
    "Note: If you are using the `netml` library, generating features with `SAMP` style options may be useful, as this option gives you time windows, and you can then simply add the segment download rate to that existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = nf.sort_values(\"datetime\")\n",
    "segment_stats = []\n",
    "\n",
    "for key, grp in nf.groupby([\"ip_src\",\"ip_dst\",\"port_src\",\"port_dst\",\"protocol\"]):\n",
    "\n",
    "    grp = grp.copy()\n",
    "    grp[\"time_gap\"] = grp[\"datetime\"].diff().dt.total_seconds()\n",
    "    grp[\"segment\"] = (grp[\"time_gap\"] >= 1.0).astype(int)\n",
    "\n",
    "    seg_count = grp[\"segment\"].sum() + 1  \n",
    "\n",
    "    flow_duration = (grp[\"datetime\"].max() - grp[\"datetime\"].min()).total_seconds()\n",
    "    seg_rate = seg_count / flow_duration\n",
    "\n",
    "    segment_stats.append({\n",
    "        \"ip_src\": key[0],\n",
    "        \"ip_dst\": key[1],\n",
    "        \"port_src\": key[2],\n",
    "        \"port_dst\": key[3],\n",
    "        \"protocol\": key[4],\n",
    "        \"segment_count\": seg_count,\n",
    "        \"segment_download_rate\": seg_rate\n",
    "    })\n",
    "\n",
    "segment_stats_df = pd.DataFrame(segment_stats)\n",
    "flow_stats = flow_stats.merge(segment_stats_df, on=[\"ip_src\",\"ip_dst\",\"port_src\",\"port_dst\",\"protocol\"], how=\"left\")\n",
    "\n",
    "print(flow_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Video Quality Inference\n",
    "\n",
    "You will now load the complete video dataset from a previous study to train and test models based on these features to automatically infer the quality of a streaming video flow.\n",
    "\n",
    "For this part of the assignment, you will need two pickle files, which we provide for you by running the code below:\n",
    "\n",
    "```\n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the File\n",
    "\n",
    "Load the video dataset pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from netml.pparser.parser import PCAP\n",
    "import ipaddress\n",
    "df = pd.read_pickle(\"netflix_dataset.pkl\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the File\n",
    "\n",
    "1. The dataset contains video resolutions that are not valid. Remove entries in the dataset that do not contain a valid video resolution. Valid resolutions are 240, 360, 480, 720, 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_res_set = [240, 360, 480, 720, 1080]\n",
    "df = df[df[\"resolution\"].isin(valid_res_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The file also contains columns that are unnecessary (in fact, unhelpful!) for performing predictions. Identify those columns, and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = []\n",
    "\n",
    "drop_cols += [c for c in df.columns if \"_id\" in c]\n",
    "drop_cols += [c for c in df.columns if \"Flag\" in c] \n",
    "drop_cols += [c for c in df.columns if \"IdleTime\" in c]\n",
    "drop_cols += [c for c in df.columns if \"index\" in c.lower()]\n",
    "drop_cols += df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(drop_cols)\n",
    "\n",
    "df_clean = df.drop(columns=drop_cols).copy()\n",
    "\n",
    "x = df_clean.drop(columns=[\"resolution\"])\n",
    "y = df_clean[\"resolution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly explain why you removed those columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We don't need IDs like home ID, deployment ID, session ID etc.\n",
    "2. synFlags, userFlags, etc. are also not part of the actual streaming\n",
    "3. Idle time is just the amount of time the client/server was idle\n",
    "4. Fields like index don't possess any useful information\n",
    "5. Any non-numerical fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data\n",
    "\n",
    "Prepare your data matrix, determine your features and labels, and perform a train-test split on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Your Model\n",
    "\n",
    "1. Select a model of your choice.\n",
    "2. Train the model using your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Your Model\n",
    "\n",
    "Perform hyperparameter tuning to find optimal parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(5, 20),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Model\n",
    "\n",
    "Evaluate your model accuracy according to the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1 Score\n",
    "3. Confusion Matrix\n",
    "4. ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"F2:\", f1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "y_prob = clf.predict_proba(x_test)\n",
    "classes = clf.classes_ \n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_bin, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
    "print(\"ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict the Ongoing Resolution of a Real Netflix Session\n",
    "\n",
    "Now that you have your model, it's time to put it in practice!\n",
    "\n",
    "Use a preprocessed Netflix video session to infer **and plot** the resolution at 10-second time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "test_raw = df.loc[x_test.index].copy()\n",
    "\n",
    "time_col = \"absolute_timestamp\" \n",
    "sess_col = \"session_id\"\n",
    "\n",
    "test_raw[time_col] = pd.to_numeric(test_raw[time_col], errors=\"coerce\").fillna(0)\n",
    "test_raw[\"time_bin\"] = (test_raw[time_col] // 10 * 10).astype(int)\n",
    "\n",
    "selected_session = test_raw[sess_col].value_counts().idxmax()\n",
    "test_session = test_raw[test_raw[sess_col] == selected_session].copy()\n",
    "print(\"Selected Session:\", selected_session)\n",
    "\n",
    "feat_cols = list(x_train.columns)\n",
    "agg_test_session = (\n",
    "    test_session.groupby(\"time_bin\")[feat_cols]\n",
    "       .median()\n",
    "       .reset_index()\n",
    ")\n",
    "x_agg_test_session = agg_test_session[feat_cols].copy()\n",
    "\n",
    "agg_test_session[\"pred_resolution\"] = best_model.predict(x_agg_test_session)\n",
    "\n",
    "# Selecting most frequest resolution in time bins for that session\n",
    "actual_resolution = (\n",
    "    test_session.groupby(\"time_bin\")[\"resolution\"]\n",
    "       .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan)\n",
    "       .reset_index()\n",
    ")\n",
    "final_plot_df = agg_one.merge(true_resolution, on=\"time_bin\", how=\"left\")\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.step(final_plot_df[\"time_bin\"], final_plot_df[\"pred_resolution\"], where=\"post\", linewidth=2, label=\"Predicted resolution\")\n",
    "plt.step(final_plot_df[\"time_bin\"], final_plot_df[\"resolution\"], where=\"post\", linestyle=\"--\", alpha=0.8, label=\"Actual resolution\")\n",
    "plt.yticks([240, 360, 480, 720, 1080])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Resolution\")\n",
    "plt.title(f\"Predicted vs actual resolution — session ID {selected_session}\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Cd1i3qtplCsVAB9qTjxA",
   "collapsed_sections": [],
   "name": "pcap_processing_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
